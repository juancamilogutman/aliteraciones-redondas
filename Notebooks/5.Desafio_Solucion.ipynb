{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-center",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-communication",
   "metadata": {},
   "source": [
    "# Preprocesamiento en Text Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-geology",
   "metadata": {},
   "source": [
    "En esta práctiva vamos a usar el dataset de mails que ya usamos en la clase de Naive Bayes.\n",
    "\n",
    "Este dataset (<a href=\"https://www.kaggle.com/riyadhrazzaq/multinomial-naive-bayes-from-scratch/data?select=spam.csv\" traget=\"_blank\">fuente</a>) tiene dos columnas:\n",
    "\n",
    "* una columna con el cuerpo del mail, \n",
    "\n",
    "* y otra con la etiqueta spam / ham según corresponda a un mail que es spam o no respectivamente.\n",
    "\n",
    "Vamos a \n",
    "* preprocesar los textos de los mails usando `CountVectorizer` y `TfidfTransformer`\n",
    "* usar Singular Value Decomposition para obtener una representación en dos dimensiones de cada mail, \n",
    "* y contruir nubes de palabras para visualizar que palabras caracterizan a cada una de la etiquetas (spam / ham).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-atlas",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-float",
   "metadata": {},
   "source": [
    "Si el import de `WordCloud` da error, pueden instalarla descomentando esta linea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -c conda-forge wordcloud=1.8.1 --yes --name dhdsblend2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-fundamentals",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Leer los datos del archivo `spam.csv`. \n",
    "\n",
    "Mantener sólo las columnas 1 y 2 etiquetarlas como 'target' y 'text'\n",
    "\n",
    "Ayuda: usar `encoding='iso8859_14'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/spam.csv\", encoding='iso8859_14')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=data.columns[2:],axis=1,inplace=True)\n",
    "data.columns=['target','text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-logging",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Los textos del dataset están en idioma inglés.\n",
    "\n",
    "Construir una lista de stems de stopwords usando `SnowballStemmer`\n",
    "\n",
    "Generar una instancia de DataFrame resultado de aplicar CountVectorizer a los textos de los mails, usando como stopwords la lista resultado del paso anterior.\n",
    "\n",
    "A partir del encoding resultado de CountVectorizer, generar una instancia de DataFrame resultado de aplicar TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english');\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# si no hacemos esto y usamos directo stopwords_sp, CountVectorizer devuelve un warning\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_en_stem, lowercase = True, strip_accents = 'unicode');\n",
    "\n",
    "vectorizer.fit(data.text);\n",
    "\n",
    "#print('Vocabulario:\\n',vectorizer.vocabulary_) # vocabulario del corpus con la frecuencia de cada término\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer_encoding = vectorizer.transform(data.text);\n",
    "\n",
    "pd.DataFrame(countvectorizer_encoding.todense(), \n",
    "             columns = vectorizer.get_feature_names()) # Usamos el método .todense() para ver la matriz completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_encoding = TfidfTransformer().fit_transform(countvectorizer_encoding);\n",
    "\n",
    "pd.DataFrame(tfidf_encoding.todense(),columns = vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-negative",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Representar en un scatterplot las dos componenetes más importantes que resultan de Singular Value Decomposition de la representación tf-idf, coloreando los puntos de acuerdo a su etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2);\n",
    "\n",
    "data_components = svd.fit_transform(tfidf_encoding)\n",
    "\n",
    "data_components_df = pd.DataFrame(data_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = data_components_df.iloc[:, 0], y = data_components_df.iloc[:, 1], hue = data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-surname",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Representar en un scatterplot las dos componenetes más importantes que resultan de Singular Value Decomposition de la representación CountVectorizer, coloreando los puntos de acuerdo a su etiqueta.\n",
    "\n",
    "¿Cuál de las dos (ej 3 vs ej 4) les resulta más informativa?\n",
    "\n",
    "¿Creen que algún algoritmo de clustering agruparía adecuadamente los tipos de email? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2);\n",
    "\n",
    "data_components_cv = svd.fit_transform(countvectorizer_encoding)\n",
    "\n",
    "data_components_cv_df = pd.DataFrame(data_components_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = data_components_cv_df.iloc[:, 0], y = data_components_cv_df.iloc[:, 1], hue = data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-sword",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "\n",
    "Usando este <a href=\"https://www.python-graph-gallery.com/wordcloud/\" target=\"_blank\">tutorial</a> o este  <a href=\"https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5\" target=\"_blank\">otro</a> generen las nubes de palabras para \n",
    "\n",
    "* los registros de etiqueta spam\n",
    "* los registros de etiqueta ham\n",
    "\n",
    "¿El resultado concuerda con la intuición que tienen sobre los textos de mails spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_spam = data.target == 'spam'\n",
    "data_spam = data.loc[mask_spam, :]\n",
    "text_spam = ' '.join(data_spam.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=1500, height=1500, margin=0,random_state=1, stopwords = stopwords_en).generate(text_spam)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ham = np.logical_not(mask_spam)\n",
    "data_ham = data.loc[mask_ham, :]\n",
    "text_ham = ' '.join(data_ham.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=1500, height=1500, margin=0, random_state=1,stopwords = stopwords_en).generate(text_ham)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_spam = WordCloud(width = 2000, height = 1500, \n",
    "                      random_state=1, background_color='steelblue', \n",
    "                      colormap='Pastel1', collocations=False, \n",
    "                      stopwords = stopwords_en).generate(text_spam)\n",
    "\n",
    "plot_cloud(wordcloud_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_ham = WordCloud(width = 2000, height = 1500, \n",
    "                      random_state=1, background_color='steelblue', \n",
    "                      colormap='Pastel1', collocations=False, \n",
    "                      stopwords = stopwords_en).generate(text_ham)\n",
    "\n",
    "plot_cloud(wordcloud_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-clock",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

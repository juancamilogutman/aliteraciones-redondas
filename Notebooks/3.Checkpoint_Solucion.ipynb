{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-phoenix",
   "metadata": {},
   "source": [
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-indie",
   "metadata": {},
   "source": [
    "# Checkpoint Preprocesamiento en Text Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-syndrome",
   "metadata": {},
   "source": [
    "En esta práctiva vamos a usar el dataset de mails que ya usamos en la clase de Naive Bayes.\n",
    "\n",
    "Este dataset (<a href=\"https://www.kaggle.com/riyadhrazzaq/multinomial-naive-bayes-from-scratch/data?select=spam.csv\" traget=\"_blank\">fuente</a>) tiene dos columnas:\n",
    "\n",
    "* una columna con el cuerpo del mail, \n",
    "\n",
    "* y otra con la etiqueta spam / ham según corresponda a un mail que es spam o no respectivamente.\n",
    "\n",
    "Vamos a preprocesar los textos de los mails usando `CountVectorizer` y `TfidfTransformer` y comparar las representaciones obtenidas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-royal",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-pharmaceutical",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Leer los datos del archivo `spam.csv`. \n",
    "\n",
    "Mantener sólo las columnas 1 y 2 etiquetarlas como 'target' y 'text'\n",
    "\n",
    "Ayuda: usar `encoding='iso8859_14'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/spam.csv\", encoding='iso8859_14')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=data.columns[2:],axis=1,inplace=True)\n",
    "data.columns=['target','text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-password",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Los textos del dataset están en idioma inglés.\n",
    "\n",
    "Construir una lista de stems de stopwords usando `SnowballStemmer`\n",
    "\n",
    "Generar una instancia de DataFrame resultado de aplicar CountVectorizer a los textos de los mails, usando como stopwords la lista resultado del paso anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords_en = stopwords.words('english');\n",
    "\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# si no hacemos esto y usamos directo stopwords_sp, CountVectorizer devuelve un warning\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_en_stem, lowercase = True, strip_accents = 'unicode');\n",
    "\n",
    "vectorizer.fit(data.text);\n",
    "\n",
    "#print('Vocabulario:\\n',vectorizer.vocabulary_) # vocabulario del corpus con la frecuencia de cada término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer_encoding = vectorizer.transform(data.text);\n",
    "\n",
    "countvectorizer_df = pd.DataFrame(countvectorizer_encoding.todense(), \n",
    "             columns = vectorizer.get_feature_names()) # Usamos el método .todense() para ver la matriz completa\n",
    "\n",
    "countvectorizer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-budget",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "A partir del encoding resultado de CountVectorizer, generar una instancia de DataFrame resultado de aplicar TfidfTransformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_encoding = TfidfTransformer().fit_transform(countvectorizer_encoding);\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_encoding.todense(),columns = vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-bruce",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Comprobemos que los valores de la representación resultado de CountVectorizer tienen rango de valores disímiles, y distintos a la representación obtenida con TfidfTransformer\n",
    "\n",
    "Para eso grafiquemos\n",
    "\n",
    "1. Mediante un scatterplot los valores medios de cada palabra de la representación resultado de CountVectorizer.\n",
    "\n",
    "2. Mediante un scatterplot los valores medios de cada palabra de la representación resultado de TfidfTransformer.\n",
    "\n",
    "3. Mediante `errorbar` https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html, \n",
    "    \n",
    "    en el eje x los valores medios de CountVectorizer de cada palabra \n",
    "    en el eje y los valores medios de TfidfTransformer de cada palabra \n",
    "    como error x los desvíos estándar de CountVectorizer de cada palabra \n",
    "    como error y los desvíos estándar de TfidfTransformer de cada palabra \n",
    "    \n",
    "Obervemos que, tal como esperamos, la dispersión es mayor en el eje x (CountVectorizer) que en el eje y (TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer_means = [np.mean(countvectorizer_df[col]) for col in countvectorizer_df.columns ]\n",
    "sns.scatterplot(x = range(len(countvectorizer_means)), y = countvectorizer_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_means = [np.mean(tfidf_df[col]) for col in tfidf_df.columns ]\n",
    "sns.scatterplot(x = range(len(tfidf_means)), y = tfidf_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer_means = [np.mean(countvectorizer_df[col]) for col in countvectorizer_df.columns]\n",
    "countvectorizer_sd = [np.std(countvectorizer_df[col]) for col in countvectorizer_df.columns]\n",
    "\n",
    "tfidf_means = [np.mean(tfidf_df[col]) for col in tfidf_df.columns]\n",
    "tfidf_sd = [np.std(tfidf_df[col]) for col in tfidf_df.columns]\n",
    "\n",
    "plt.errorbar(x = countvectorizer_means, y = tfidf_means, yerr = tfidf_sd, xerr = tfidf_sd, linestyle='None', marker='^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-missouri",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-presence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

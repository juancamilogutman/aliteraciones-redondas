{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%run \"../../../common/0_notebooks_base_setup.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"../../../common/dhds.css\">\n",
    "<div class=\"Table\">\n",
    "    <div class=\"Row\">\n",
    "        <div class=\"Cell grey left\"> <img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_portada.jpg\" align=\"center\" width=\"90%\"/></div>        \n",
    "        <div class=\"Cell right\">\n",
    "            <div class=\"div-logo\"><img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/common/logo_DH.png\" align=\"center\" width=70% /></div>\n",
    "            <div class=\"div-curso\">DATA SCIENCE</div>\n",
    "            <div class=\"div-modulo\">MÓDULO 6</div>\n",
    "            <div class=\"div-contenido\">Text Minig <br/> Preprocesamiento de texto</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "---\n",
    "\n",
    "- Introducción\n",
    "\n",
    "- Bag of words\n",
    "\n",
    "- Corpus y documentos\n",
    "\n",
    "- Tokenización\n",
    "\n",
    "- Construcción de un vocabulario\n",
    "\n",
    "- Stemming y Lemmatization \n",
    "\n",
    "- Encoding\n",
    "\n",
    "- Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Introducción\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es Text Mining?\n",
    "\n",
    "---\n",
    "\n",
    "Text Mining es el conjunto de **técnicas que se utilizan para explorar grandes cantidades de texto**, de manera automática o semiautomática, para descubrir patrones repetitivos, tendencias o reglas que explican el comportamiento del texto.\n",
    "\n",
    "El objetivo es ayudar a **comprender el contenido de un conjunto de textos** por medio de estadísticas y algoritmos de búsqueda.\n",
    "\n",
    "En esta clase **analizaremos datos de tipo texto**. Algo hemos hecho ya cuando preprocesamos datasets usando, por ejemplo, expresiones regulares. Lo que hacíamos en ese entonces era buscar algo que ya sabíamos que estaba allí. La tarea que nos ocupa ahora es diferente. Queremos usar los **algoritmos de machine learning que aprendimos para hacer clasificación, pero ahora con textos**.\n",
    "\n",
    "Existen múltiples escenarios en los cuales querríamos hacer esto. Uno de los primeros problemas de aplicación de machine learning, de hecho, fue el de clasificación de correos como spam o no-spam. Otros casos son la clasificación de comentarios de usuarios sobre algún producto como positivos o negativos, identificar a qué sección de un diario pertenecen distintas notas, etc. \n",
    "\n",
    "En resumen, **queremos extraer información sobre el contenido de los textos**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Técnicas de text mining y su funcionalidad\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_problems_techniques.png\" align=\"center\" />\n",
    "\n",
    "Fuente: <a href=\"https://www.researchgate.net/publication/311394659_Text_Mining_Techniques_Applications_and_Issues\" target=\"_blank\">Text Mining: Techniques, Applications and Issues\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Clasificación de documentos: clasificación de texto, estandarización de elementos \n",
    "\n",
    "* Recuperación de información: búsqueda de palabras clave/ consulta e indexación\n",
    "\n",
    "* Agrupación de documentos: agrupación de frases\n",
    "\n",
    "* Procesamiento del lenguaje natural: corrección ortográfica, lematización, análisis gramatical y desambiguación del sentido de las palabras\n",
    "\n",
    "* Extracción de información: extracción de relaciones / análisis de enlaces\n",
    "\n",
    "* Web mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Features\n",
    "\n",
    "---\n",
    "\n",
    "Para poder implementar los modelos que ya conocemos, **necesitamos representar los datos como una matriz de features**. \n",
    "\n",
    "En el caso de modelos de aprendizaje supervisado, necesitamos además una etiqueta.\n",
    "\n",
    "<b>¿Cómo convertirían un texto en una matriz de features?</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La propuesta más simple es contar la ocurrencia de los términos que aparecen en los textos.\n",
    "\n",
    "Vamos a representar un texto como una **matriz donde cada fila es un documento y cada columna una palabra**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## El problema de la dimensionalidad\n",
    "\n",
    "---\n",
    "\n",
    "Los textos son secuencias de palabras (y signos de puntuación!) y el sentido de los mismos está contenido precisamente en las estructuras semánticas que forman estos elementos combinados. \n",
    "\n",
    "**El sentido no está dado por la mera presencia de las palabras, sino por cómo se articulan**.\n",
    "\n",
    "Queremos buscar una estructura en el *espacio de los textos posibles*. \n",
    "\n",
    "El problema es que ese **espacio es de una dimensionalidad enorme**. \n",
    "\n",
    "Para poder computar métricas de distancia entre los textos, para poder entrenar algoritmos de machine learning que nos permitan encontrar patrones en los datos, necesitamos **definir representaciones reducidas de los textos**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Bag of words\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words\n",
    "\n",
    "---\n",
    "\n",
    "Una de las maneras más simples y efectivas de representar los textos es la que se conoce como **\"bag of words\"**\n",
    "\n",
    "Consiste en descartar la mayor parte de la estructura de los textos como párrafos, capítulos, etc, y conservar únicamente el conjunto de palabras y el número de veces que aparecen en el texto. \n",
    "\n",
    "Es decir, olvidamos el orden en que aparecen. Matemáticamente, el número de maneras en que podemos ordenar n elementos se calcula como n! (se lee 'factorial de n' o 'n factorial') y vale:\n",
    "\n",
    "$n!=n*(n-1)*(n-2)*(n-3)*...*3*2*1$\n",
    "\n",
    "Es decir que si tuviéramos 10 palabras diferentes para construir un texto (una oración), podríamos construir $3628800$ textos distintos ya que\n",
    "\n",
    "$10!=10*9*8*7*6*5*4*3*2*1 = 3628800 $ \n",
    "\n",
    "Pero en el esquema \"bag of words\" todos ellos estarían representados de la misma manera (la misma bolsa de palabras), de manera que serían textos indistinguibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Corpus y documentos\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus y documentos\n",
    "\n",
    "---\n",
    "\n",
    "Llamaremos **corpus** a un conjunto de textos y **documento** a cada texto que compone el corpus (puede ser un libro, un twit o el comentario de un usuario) y que a la vez es nuestra unidad de dato (sería una fila en un dataframe).\n",
    "\n",
    "Computar la representación \"bag of words\" de un corpus de documentos conlleva tres pasos:\n",
    "\n",
    "1. **Tokenización**: convertir cada documento a una lista de palabras (y signos de puntuación) que lo componen.\n",
    "\n",
    "2. **Construcción de un vocabulario**: colectar todas las palabras que se registraron en el corpus y ordenarlas (típicamente por orden alfabético).\n",
    "\n",
    "3. **Encoding**: representar los documentos como vectores en el espacio de las palabras del vocabulario.\n",
    "\n",
    "Veremos cada uno de estos pasos en detalle y luego incorporaremos herramientas de sklearn que permiten aglutinar todo en un sólo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.nltk.org\" target=\"_blank\">NLTK (Natural Language Toolkit)</a> es una librería de python de código abierto para el procesamiento de lenguaje natural.\n",
    "\n",
    "Tiene un libro online gratuito para consultar:\n",
    "\n",
    "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O’Reilly Media Inc. http://nltk.org/book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Tokenización\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenización\n",
    "\n",
    "---\n",
    "\n",
    "Los **tokens son típicamente las palabras y signos de puntuación**. \n",
    "\n",
    "Tokenización es la **transformación de un texto a unidades constitutivas llamadas tokens**. \n",
    "\n",
    "La librería NLTK cuenta con herramientas para hacer ésto, identificando los signos de puntuación: cuándo los mismos separan oraciones y cuándo cumplen otra función, como una abreviatura.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')    # esta linea deben ejecutarla sólo una vez\n",
    "nltk.download('stopwords') # esta linea deben ejecutarla sólo una vez\n",
    "nltk.download('wordnet')    # esta linea deben ejecutarla sólo una vez\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "print(word_tokenize(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por otro lado, si quisiéramos obtener una lista de oraciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. \\\n",
    "    Is this the third sentence? Yes, it is!\"\n",
    "\n",
    "print('Tokenizamos usando sent_tokenize:')\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Es posible que en algunos casos el tokenizador falle, por ejemplo no reconociendo una abreviatura. \n",
    "\n",
    "En el siguiente ejemplo el tokenizador no reconoce 'al.' de modo que interpreta el punto como el fin de una oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text2 = \"According to Hotho et al. (2005) we can differ three different perspectives of text mining, \\\n",
    "namely text mining as information extraction, text mining as text data mining, and text mining as KDD \\\n",
    "(Knowledge Discovery in Databases) process. Text mining is 'the discovery by computer of new, \\\n",
    "previously unknown information, by automatically extracting information from different written resources.\"\n",
    "\n",
    "print(sent_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos resolver este problema incluyendo abreviaturas al tokenizador. \n",
    "\n",
    "Para ello, en lugar de importar el método sent_tokenize, instanciamos los objetos `PunkTrainer` y `PunktSentenceTokenizer`. \n",
    "\n",
    "(La idea de la siguiente celda sólo es mostrar un ejemplo. Pueden consultar la documentación <a href=\"http://www.nltk.org/api/nltk.tokenize.html?highlight=tokenizer#nltk.tokenize.punkt.PunktTrainer\" target=\"_blank\">aquí</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "\n",
    "tokenizer._params.abbrev_types.add('al')\n",
    "\n",
    "print(tokenizer.tokenize(text2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Construcción de un vocabulario\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Construcción de un vocabulario\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a **representar los textos como una bolsa de palabras**. \n",
    "\n",
    "Podríamos tokenizar todos los documentos y definir el vocabulario como el set de palabras que aparecieron al menos una vez en todo el corpus.\n",
    "\n",
    "Esto tiene el problema de que el número de palabras será muy grande y muchas de ellas serán muy poco informativas sobre el contenido del texto, por ejemplo las preposiciones, pronombres, etc. \n",
    "\n",
    "A estas palabras se las llama **stopwords** y a menudo se las **excluye del vocabulario**. \n",
    "\n",
    "Otra técnica para reducir la dimensionalidad del problema consiste en **agrupar palabras que comparten la misma raíz etimológica** como \"correr\", \"corriendo\", \"corredor\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stopwords\n",
    "\n",
    "---\n",
    "\n",
    "NLTK tiene listas de stopwords en distintos idiomas, podemos acceder a las mismas del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stopwords_sp = stopwords.words('spanish');\n",
    "\n",
    "print('\\n Las primeras 20 en español:')\n",
    "print(stopwords_sp[:20])\n",
    "\n",
    "stopwords_en = stopwords.words('english');\n",
    "\n",
    "print('\\n Las primeras 20 en ingles:')\n",
    "print(stopwords_en[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemming y Lemmatization \n",
    "\n",
    "---\n",
    "\n",
    "Stemming y lemmatization son **maneras de reducir las palabras a su raíz etimológica**.\n",
    "\n",
    "**Stemming lo hace por sustracción de sufijos y prefijos de las palabras**. \n",
    "\n",
    "La raíz que queda **(stem) muchas veces no es una palabra en sí misma**. \n",
    "\n",
    "Por ejemplo al pasar la palabra \"corriendo\" por un stemmer obtenemos \"corr\". \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_stemming.png\" align=\"center\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por el contrario, **el lematizador siempre devuelve una versión reducida de la palabra (lema), pero que es en sí misma una palabra de la misma familia**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_lemmatization.png\" align=\"center\" />\n",
    "\n",
    "Para profundizar:\n",
    "<a href=\"https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\">Stemming and Lemmatization in Python</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Importamos los algoritmos de Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Creamos objects a partir de las clases PorterStemmer y LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Le pasamos palabras a ambos algoritmos para que hagan stemming:\n",
    "print(\"Porter Stemmer\")\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(\"\\n\")\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos una lista de palabras para hacer stemming con ambos algoritmos:\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\", \\\n",
    "             \"stabil\",\"destabilize\",\"misunderstanding\",\\\n",
    "             \"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stemmers en otros idiomas:\n",
    "\n",
    "Python **nltk** no solo proporciona dos stemmers en inglés: PorterStemmer y LancasterStemmer, sino también muchos stemmers que no están en inglés como **SnowballStemmers**.\n",
    "\n",
    "Idiomas que maneja **SnowballStemmers**:\n",
    "\n",
    " - danés\n",
    " - holandés\n",
    " - inglés\n",
    " - francés\n",
    " - alemán\n",
    " - húngaro\n",
    " - italiano\n",
    " - noruego\n",
    " - portugués\n",
    " - rumano\n",
    " - ruso\n",
    " - español\n",
    " - sueco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "word = 'having';\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "print(\"{0:15}{1:10}\".format(word, englishStemmer.stem(word)))\n",
    "\n",
    "palabra = 'corriendo';\n",
    "spanishStemmer=SnowballStemmer(\"spanish\")\n",
    "print(\"{0:15}{1:10}\".format(palabra, spanishStemmer.stem(palabra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has \\\n",
    "bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n{0:20}{1:20}{2:20}\\n\".format(\"Word\",\"Lemma\",'Stem'))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}{2:20}\".format(word, wordnet_lemmatizer.lemmatize(word), englishStemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Encoding\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding\n",
    "\n",
    "---\n",
    "\n",
    "Podemos representar a los documentos en función de las palabras que los componen, sin considerar las estructuras semánticas que éstas forman. \n",
    "\n",
    "De esta manera, **un documento puede representarse como un vector en el espacio de palabras que conforman el vocabulario**. \n",
    "\n",
    "Existen diferentes maneras de definir estos vectores. La más intuitiva es contar el número de veces que aparece cada palabra en un documento y asignarlo como la coordenada o el peso correspondiente a dicha palabra en el vector. \n",
    "\n",
    "<a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\" target=\"_blank\">CountVectorizer</a> provee esta funcionalidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `CountVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "Al momento de instanciarla, permite pasarle una lista de stopwords para que no las tenga en cuenta.\n",
    "\n",
    "Como todos los modelos de sklearn, tiene los métodos fit y transform. \n",
    "\n",
    "`fit` genera el vocabulario a partir de los documentos, y `transform` vectoriza los documentos al espacio del vocabulario.\n",
    "\n",
    "Como típicamente el vocabulario es muy grande, las matrices son muy esparsas (tienen muchos ceros) por lo que es conveniente almacenar sólamente las entradas no nulas de las mismas en un objeto de la clase \"sparse matrix\". `CountVectorizer` genera matrices de esta clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos textos \n",
    "\n",
    "t0 = \"El potro y el angel llegaron al cine por casualidad.\"\n",
    "t1 = \"El ángel, el tanque del cine nacional, un paso más cerca del oscar\"\n",
    "t2 = \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\"\n",
    "t3 = \"Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.\"\n",
    "t4 = \"Juan Martin del potro fue recibido por una multitud en Tandil.\"\n",
    "t5=  \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "textos=[t0,t1,t2,t3,t4,t5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stopwords_sp = stopwords.words('spanish');\n",
    "\n",
    "# si no hacemos esto y usamos directo stopwords_sp, CountVectorizer devuelve un warning\n",
    "stopwords_sp_stem = [spanishStemmer.stem(x) for x in stopwords_sp]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_sp_stem, lowercase = True, strip_accents = 'unicode');\n",
    "\n",
    "vectorizer.fit(textos);\n",
    "print('Vocabulario:\\n',vectorizer.vocabulary_) # vocabulario del corpus con la frecuencia de cada término\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "countvectorizer_encoding = vectorizer.transform(textos);\n",
    "print('\\n Transformamos los textos a una matriz esparsa:',type(countvectorizer_encoding))\n",
    "\n",
    "pd.DataFrame(countvectorizer_encoding.todense(), \n",
    "             columns = vectorizer.get_feature_names()) # Usamos el método .todense() para ver la matriz completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "---\n",
    "\n",
    "La representación de los textos generada mediante `CountVectorizer` tiene en cuenta cuántas veces se observó cada término del vocabulario en cada documento. \n",
    "\n",
    "**Problemas**: \n",
    "\n",
    "* En un documento mucho más largo que los otros, el conteo de palabras puede resultar en números mucho mayores que en el resto de los documentos. \n",
    "\n",
    "    Para corregir esta anomalía, deberíamos normalizar (dividir) el conteo de cada palabra por el tamaño de cada documento.\n",
    "\n",
    "* Hay palabras que aparecen en muchos documentos y resultan entonces poco informativas para distinguirlos. \n",
    "\n",
    "    Una palabra que aparece muchas veces en un documento, pero pocas veces en los demás, es una palabra muy distintiva de ese documento y será importante para representarlo, mientras que palabras que aparecen pocas veces, o que aparecen en muchos documentos serán menos informativas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La transformación TF-IDF tiene en cuenta estos factores de la siguiente manera:\n",
    "\n",
    "* El término 't' dentro del documento 'd' tiene un coeficiente tf-idf(t,d) que es el producto de dos factores:\n",
    "\n",
    "\\begin{equation}\n",
    " \\text{tf-idf}(t,d)=\\text{tf}(t,d) \\times idf(t)\n",
    "\\end{equation}\n",
    "\n",
    "* tf(t,d) es la frecuencia de aparición de t dentro de d (normalizada). \n",
    "\n",
    "*  idf(t) es la *inverse document frecuency* del término t y se calcula como:\n",
    "\\begin{equation}\n",
    "    \\text{idf}(t)=\\log{\\frac{N}{\\text{df(t)+1}}} \n",
    "\\end{equation}\n",
    "\n",
    "en donde \n",
    "\n",
    "* N es el número de documentos \n",
    "\n",
    "* df(t) es el número de documentos en los que aparece el término t. \n",
    "\n",
    "* Se suele sumar 1 en el denominador para no tener problemas si existe un término en el vocabulario que no aparece en ningún documento (df=0). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ejemplo\n",
    "\n",
    "---\n",
    "\n",
    "document1 = 'el potro y el angel llegaron al cine por casualidad'\n",
    "\n",
    "document2 = 'el angel el tanque del cine nacional un paso mas cerca del oscar'\n",
    "\n",
    "document3 = 'final del mes del cine nacional el angel la unica cinta millonaria'\n",
    "\n",
    "document4 = 'juan martin del potro volvio a tandil se dio el ultimo bano de masas con los suyos'\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    &\\text{tf('nacional', document1)}=\\frac{1}{10}=0.1\\\\\n",
    "    &\\text{tf('nacional', document2)}=\\frac{1}{13}=0.077\\\\\n",
    "    &\\text{tf('nacional', document3)}=\\frac{0}{12}=0\\\\\n",
    "    &\\text{tf('nacional', document4)}=\\frac{0}{17}=0 \\\\\n",
    "    &\\text{idf('nacional')}=\\log{\\frac{4}{3}}=0.288\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y los coeficientes tf-idf:\n",
    "\n",
    "\\begin{align}\n",
    "    &\\text{tf-idf('nacional',doc1)}=0.1\\times 0.288=0.0288 \\\\\n",
    "    &\\text{tf-idf('nacional',doc2)}=0.077\\times 0.288=0.0222 \\\\\n",
    "    &\\text{tf-idf('nacional',doc3)}=0\\times 0.288=0 \\\\\n",
    "    &\\text{tf-idf('nacional',doc4)}=0\\times 0.288=0\n",
    "\\end{align}\n",
    "\n",
    "El coeficiente tf-idf para \"potro\" en el documento 1 da:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{tf-idf('potro',doc1)}= \\frac{1}{10} \\times \\log{\\frac{4}{2}}=0.07\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Podemos ver que si bien \"potro\" y \"nacional\" aparecen el mismo número de veces dentro del documento 1, \"potro\" tiene un coeficiente mayor porque no aparece en ningún otro documento. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_tf-idf.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "En sklearn, los objetos de la clase TfidfTransformer sirven para transformar una matriz generada por CountVectorizer(). \n",
    "\n",
    "Podemos saltearnos el uso de countvectorizer y usar directamente TfidfVectorizer.\n",
    "\n",
    "A diferencia de la estimación clásica del factor idf que vimos más arriba, TfidfVectorizer y TfidfTransformer por defecto calculan una versión suavizada del idf como\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{idf(t)}= \\log{\\frac{N+1}{df(t)+1}}+1\n",
    "\\end{equation}\n",
    "\n",
    "y luego normaliza los documentos vectorizados por su norma L2. (ver https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "Tfidf_encoding=TfidfTransformer().fit_transform(countvectorizer_encoding);\n",
    "\n",
    "pd.DataFrame(Tfidf_encoding.todense(),columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Singular Value Decomposition (SVD)\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "---\n",
    "\n",
    "SVD es una técnica (general) de factorización de matrices.\n",
    "\n",
    "Comprender la matemática involucrada excede las ambiciones de esta presentación. Para indagar en ella pueden consultar alguna de estas referencias:\n",
    "\n",
    "*Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications*\n",
    "2012, Chapter 11.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "\n",
    "En particular **en text mining usamos SVD para reducir la dimensionalidad del corpus de texto** que, a diferencia de lo que hemos hecho hasta ahora, no consiste en remover elementos (stopwords, etc), sino en **encontrar combinaciones de palabras que resulten informativas y quedarnos con \"las mejores\" de éstas**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una analogía posible es la siguiente: \n",
    "\n",
    "Podemos describir un **rectángulo dando dos variables (features): su base y su altura**. \n",
    "\n",
    "Si quisiéramos describir el rectángulo con una sola de estas features, por ejemplo la base, estaríamos perdiendo información muy relevante ya que existen rectángulos muy diferentes con la misma base. \n",
    "\n",
    "Sin embargo, **si generáramos una nueva variable \"área\" igual al producto de base por altura**, y describiéramos al rectángulo usando solamente el área, **estaríamos reduciendo la dimensionalidad de una manera mucho más razonable**, guardando más información sobre el rectángulo original que al quedarnos solo con la base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**SVD es una transformación algebraica parecida a PCA** (principal component analysis) que se puede usar en el contexto de text mining **para encontrar combinaciones lineales de los términos que resulten informativos**, de modo que podamos describir el data set con un número de combinaciones menor al número de términos que teníamos originalmente. \n",
    "\n",
    "Estas combinaciones pueden considerarse como **dimensiones con *sentido semántico latente* (latent semantic dimensions)**, es decir, dimensiones en las que tiene sentido proyectar el dataset precisamente por su contenido semántico.\n",
    "\n",
    "El motivo por el cual podemos reducir la dimensionalidad de los textos proyectandolos a estas *latent semantic dimensions* es que **muchas veces existe redundancia en el conjunto de documentos**. Es decir que con palabras más o menos distintas, muchos documentos hablan de los mismos temas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En el ejemplo de los 6 textos que venimos usando \n",
    "\n",
    "    t0='El potro y el angel llegaron al cine por casualidad.'\n",
    "    t1= 'El ángel, el tanque del cine nacional, un paso más cerca del oscar',\n",
    "    t2= \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\",\n",
    "    t3= 'Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.',\n",
    "    t4= 'Juan Martin del potro fue recibido por una multitud en Tandil.',\n",
    "    t5= \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "\n",
    "\n",
    "Hay 45 palabras distintas. (Antes redujimos el número de términos a 28 quitando stopwords). \n",
    "\n",
    "Sin embargo los textos hablan esencialmente de tres temas: hay dos películas nuevas en el cine, Del Potro visitó Tandil, Del Potro fue al cine. \n",
    "\n",
    "Esta reducción de la dimensionalidad podría mejorar la performance de un clasificador o un modelo de clustering. Por otro lado, una reducción a dos o tres dimensiones nos puede permitir visualizar los datos. \n",
    "\n",
    "Hay que tener en cuenta, sin embargo, que **en general necesitaremos más dimensiones para describir correctamente el corpus y es posible que la representación en dos dimensiones no nos revele mucho sobre la estructura del dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2);\n",
    "P = svd.fit_transform(Tfidf_encoding)\n",
    "\n",
    "#grafico\n",
    "color = ['m', 'g', 'r', 'c', 'b','k']\n",
    "plt.figure()\n",
    "patches = []\n",
    "\n",
    "for i,texto in enumerate(textos):\n",
    "    plt.plot(P[i,0], P[i,1], color[i]+\"o\")\n",
    "    patches.append(mpatches.Patch(color=color[i], label='t'+str(i)))\n",
    "\n",
    "plt.legend(handles=patches)\n",
    "plt.xlabel('dimension 1')\n",
    "plt.ylabel('dimension 2')\n",
    "#plt.axis([-4, 4, -4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Las dimensiones que generamos son combinaciones lineales de los términos. \n",
    "\n",
    "Podemos ver cuánto pesa cada término en la definición de estas dimensiones. \n",
    "\n",
    "Ordenemos los términos en función de cuánto pesan en cada dimensión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# coeficientes (pesos) de los términos en cada una de las dos dimensiones\n",
    "comp1,comp2 = svd.components_ \n",
    "\n",
    "# los ordenamos de menor a mayor y nos quedamos con los índices de sus posiciones en el array\n",
    "indices=np.argsort(comp1); \n",
    "\n",
    "# invertimos para que queden ordenados de mayor a menor\n",
    "indices=indices[::-1] \n",
    "\n",
    "# Evaluamos los términos en estas posiciones\n",
    "print('Dimension 1:')\n",
    "print(np.array(vectorizer.get_feature_names())[indices]) \n",
    "\n",
    "print('\\n')\n",
    "\n",
    "indices=np.argsort(comp2);\n",
    "indices=indices[::-1]\n",
    "print('Dimension 2:')\n",
    "print(np.array(vectorizer.get_feature_names())[indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "En la representación 2D de los textos vemos que hay dos grupos separados: uno que habla escencialmente de cine y otro que habla de tenis. \n",
    "\n",
    "En un escenario de aprendizaje no supervisado, podríamos encontrar estos grupos mediante un algoritmo de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Conclusiones\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "En esta clase presentamos \n",
    "\n",
    "* Conceptos y técnicas útiles de preprocesamiento de texto para construir features que permitan modelar problemas de aprendizaje supervisado y no supervisado.\n",
    "\n",
    "* <a href=\"https://www.nltk.org\" target=\"_blank\">NLTK (Natural Language Toolkit)</a>, una librería de python de código abierto para el procesamiento de lenguaje natural.\n",
    "\n",
    "* Reducción de dimensionalidad mediante Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Hands-on\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_46_Text_Mining_1/Presentacion/img/M6_CLASE_46_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hands-on\n",
    "\n",
    "---\n",
    "\n",
    "Usando las letras de canciones en español disponibles <a href=\"https://www.kaggle.com/smunoz3801/9325-letras-de-rap-en-espaol\" target=\"_blank\">aquí</a>, construir una nube de palabras para identificar cuáles son las más frecuentes.\n",
    "\n",
    "Para eso vamos a usar la bilbioteca `WordCloud` que pueden instalarla descomentando esta linea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#! conda install -c conda-forge wordcloud=1.8.1 --yes --name dhdsblend2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Construimos la nube de palabras con \n",
    "\n",
    "`wordcloud = WordCloud(width=1500, height=1500, margin=0, stopwords = mis_stop_word).generate(mi_texto)`\n",
    "\n",
    "Y la graficamos con el método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Ejercicio 1\n",
    "\n",
    "Leer en un dataframe los datos del archivo `hhgroups_merge_28_05.zip`.\n",
    "\n",
    "Fuente: https://www.kaggle.com/smunoz3801/9325-letras-de-rap-en-espaol\n",
    "\n",
    "Consideraremos \n",
    "* documento al valor del campo `letra` de cada uno de los registros que componen el dataframe \n",
    "* corpus al conjunto de documentos, es decir el conjunto de valores del campo `letra`\n",
    "\n",
    "### Ejercicio 2\n",
    "\n",
    "Tokenizar el corpus, hacer stemming, eliminar stopwords \n",
    "\n",
    "Considerar \"estribillo\" una stopword (aparece en muchos registros y va a ser muy relevante en la nube de palabras).\n",
    "\n",
    "### Ejercicio 3\n",
    "\n",
    "Construir una nube de palabras para visualizar las palabras más frecuentes en las letras de estas canciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solución\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/hhgroups_merge_28_05.zip\", sep=\",\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = data.letra\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tokenizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "\n",
    "flat_tokens[0:5]\n",
    "\n",
    "#tokens_joined = ' '.join(tokens)\n",
    "#tokens_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "spanish_stemmer = SnowballStemmer('spanish', ignore_stopwords = False)\n",
    "\n",
    "stem_tokens = [spanish_stemmer.stem(x) for x in flat_tokens]\n",
    "\n",
    "print(\"cantidad de stem_tokens: \", len(stem_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('spanish');\n",
    "\n",
    "stopwords_en.append(\"estribillo\")\n",
    "\n",
    "stopwords_es_stem = [spanish_stemmer.stem(x) for x in stopwords_en]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stem_tokens_non_stopwords = [x for x in stem_tokens if x not in stopwords_es_stem]\n",
    "\n",
    "print(\"cantidad de stem_tokens sin stopwords: \", len(stem_tokens_non_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejercicio 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# no pasamos la lista de stopwords porque ya las quitamos en el ejercicio anterior:\n",
    "\n",
    "mi_texto = ' '.join(stem_tokens_non_stopwords) \n",
    "\n",
    "mywordcloud = WordCloud(width=1500, height=1500, margin=0, stopwords = None).generate(mi_texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_cloud(mywordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"div-dhds-fondo-1\"> Referencias y material adicional\n",
    "<img src=\"https://raw.githubusercontent.com/Digital-House-DATA/ds_blend_2021_img/master/M6/CLASE_44_Descenso_Gradiente/Presentacion/img/M6_CLASE_44_separador.png\" align=\"center\" />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"http://nltk.org/book\" target=\"_blank\">Natural Language Processing with Python.</a> Steven Bird, Ewan Klein, and Edward Loper (2009). O’Reilly Media Inc. \n",
    "\n",
    "<a href=\"https://www.researchgate.net/publication/311394659_Text_Mining_Techniques_Applications_and_Issues/fulltext/5844091808ae8e63e625730a/Text-Mining-Techniques-Applications-and-Issues.pdf\" target=\"_blank\" />Text Mining: Techniques, Applications and Issues</a>\n",
    "\n",
    "<a href=\"https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/\" target=\"_blank\">How tokenizing text, sentence, words works</a>\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558\" target=\"_blank\">TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
